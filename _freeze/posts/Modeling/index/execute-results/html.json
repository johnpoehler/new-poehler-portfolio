{
  "hash": "f1ee1c1e9a326950e47535a3128f4238",
  "result": {
    "markdown": "---\ntitle: \"Modeling the Data\"\n\nauthor: \"John Poehler\"\n\ndate: \"2023-08-19\"\n\ncategories: [modelling, regression]\n\nimage: \"regression.jpg\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressMessages(library(dplyr))\nlibrary(dplyr)\nNBA<-read.csv(\"nbastatz1.csv\",header=TRUE)\n\nNBA[NBA==\"\"]<-NA\nNBA$Pos <- replace(NBA$Pos,NBA$Pos == \"G-F\",\"G\")\nNBA$Pos <- replace(NBA$Pos,NBA$Pos == \"F-G\",\"F\")\nNBA$Pos <- replace(NBA$Pos,NBA$Pos == \"C-F\",\"C\")\nNBA$Pos <- replace(NBA$Pos,NBA$Pos == \"F-C\",\"F\")\nNBA$Player[NBA$Player == \"Jos\\xe9 Calder\\xf3n\"]<- \"Jose Calderon\"\nNBA$Player[NBA$Player == \"Manu Gin\\xf3bili\"]<- \"Manu Ginobili\"\nNBA$Player[NBA$Player == \"Hedo T\\xfcrko?lu\"]<- \"Hedo Turkoglu\"\nNBA$Player[NBA$Player == \"Nen\\xea\"]<- \"Nene\"\n```\n:::\n\n\n# Let's build and run a linear regression model!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nwin.NBA <- NBA[c('WS','Age','G','MP','FG','FGA','X2P','X2PA','X3P','X3PA','FT','FTA','ORB','DRB','TRB','AST','STL','BLK','TOV','PF','PTS','FG.','X2P.','FT.','TS.','eFG.','Win.')]\n\nset.seed(123)\n\ntrain_index <- createDataPartition(win.NBA$Win., p = 0.7, list = FALSE)\ntrain_data <- win.NBA[train_index, ]\ntest_data <- win.NBA[-train_index, ]\n\n\nwin.percent.regression <- lm(Win. ~ ., data = train_data)\n\n\nsummary(win.percent.regression)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Win. ~ ., data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.28236 -0.05548  0.00397  0.05699  0.33542 \n\nCoefficients: (4 not defined because of singularities)\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.630e-01  2.012e-01   4.289 2.06e-05 ***\nWS           4.148e-02  2.837e-03  14.621  < 2e-16 ***\nAge          3.338e-03  9.789e-04   3.410 0.000689 ***\nG            3.299e-03  9.421e-04   3.502 0.000493 ***\nMP          -1.409e-04  1.984e-05  -7.103 3.10e-12 ***\nFG           4.500e-04  7.081e-04   0.636 0.525301    \nFGA         -3.217e-04  2.681e-04  -1.200 0.230476    \nX2P         -9.625e-04  7.142e-04  -1.348 0.178225    \nX2PA         5.454e-04  2.772e-04   1.968 0.049503 *  \nX3P                 NA         NA      NA       NA    \nX3PA                NA         NA      NA       NA    \nFT          -5.762e-04  2.786e-04  -2.068 0.039024 *  \nFTA          8.909e-05  1.816e-04   0.490 0.623987    \nORB         -4.069e-04  6.862e-05  -5.930 4.84e-09 ***\nDRB         -1.443e-04  3.339e-05  -4.322 1.78e-05 ***\nTRB                 NA         NA      NA       NA    \nAST         -2.539e-04  3.685e-05  -6.890 1.28e-11 ***\nSTL         -3.447e-04  1.130e-04  -3.049 0.002383 ** \nBLK         -1.663e-04  7.498e-05  -2.218 0.026896 *  \nTOV          9.301e-04  1.293e-04   7.195 1.66e-12 ***\nPF          -6.414e-05  9.297e-05  -0.690 0.490476    \nPTS                 NA         NA      NA       NA    \nFG.         -4.855e-01  5.065e-01  -0.959 0.338086    \nX2P.         1.310e+00  4.842e-01   2.704 0.007014 ** \nFT.          2.455e-02  1.288e-01   0.191 0.848943    \nTS.         -9.870e-01  7.467e-01  -1.322 0.186666    \neFG.        -4.009e-01  7.866e-01  -0.510 0.610491    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08659 on 678 degrees of freedom\nMultiple R-squared:  0.3635,\tAdjusted R-squared:  0.3428 \nF-statistic:  17.6 on 22 and 678 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n# Now, a logistic regression model!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(caret)\n\nindyNBA<- NBA[c('WS','Age','G','MP','FG','FGA','X2P','X2PA','X3P','X3PA','FT','FTA','ORB','DRB','TRB','AST','STL','BLK','TOV','PF','PTS','FG.','X2P.','X3P.','FT.','TS.','eFG.','Champ.')]\n\nset.seed(123)\n\nindex <- createDataPartition(indyNBA$Champ., p = 0.7, list = FALSE)\n\ntrain <- indyNBA[index, ]\ntest <- indyNBA[-index, ]\n\nlogit_model <- glm(Champ. ~ WS + MP +DRB + AST + STL +\n                   TOV ,\n                   data = train, family = binomial)\n\nsummary(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Champ. ~ WS + MP + DRB + AST + STL + TOV, family = binomial, \n    data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9818  -0.4455  -0.3274  -0.2438   2.6761  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.2115528  1.0080328  -0.210    0.834    \nWS           0.3731998  0.0707348   5.276 1.32e-07 ***\nMP          -0.0020999  0.0004737  -4.433 9.31e-06 ***\nDRB         -0.0008053  0.0010494  -0.767    0.443    \nAST         -0.0002387  0.0010324  -0.231    0.817    \nSTL          0.0002486  0.0043261   0.057    0.954    \nTOV         -0.0015962  0.0033557  -0.476    0.634    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 414.23  on 699  degrees of freedom\nResidual deviance: 372.35  on 693  degrees of freedom\nAIC: 386.35\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\ntest$predicted_champ <- predict(logit_model, newdata = test, type = \"response\")\n\ntest$predicted_champ_binary <- ifelse(test$predicted_champ >= 0.5, \"yes\", \"no\")\n\nconfusion_matrix <- table(test$Champ., test$predicted_champ_binary)\nprint(confusion_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n     no yes\n  0 269   3\n  1  26   2\n```\n:::\n\n```{.r .cell-code}\naccuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", accuracy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 0.903333333333333\"\n```\n:::\n:::\n\n\n# **Lastly, a LASSO Regression model!**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-7\n```\n:::\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(caTools)\n\n\nsubsetNBA <- select(NBA, WS, Age, G, MP, FG, FGA, X2P, X2PA, X3P, X3PA, FT, FTA, ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PTS, `FG.`, `X2P.`, `FT.`, `TS.`, `eFG.`)\n\n\nset.seed(345)  \nsample <- sample.split(subsetNBA$WS, SplitRatio = 0.70)\ntrain <- subsetNBA[sample, ]\ntest <- subsetNBA[!sample, ]\n\n\nx <- as.matrix(train[, -1])  \ny <- train$WS\n\nlasso_model <- glmnet(x, y, alpha = 1)  \n\nlambda_seq <- exp(seq(log(0.01), log(1), length = 100))  \n\n\ncv_model <- cv.glmnet(x, y, alpha = 1, lambda = lambda_seq)\n\n\nopt_lambda <- cv_model$lambda.min\n\n\ncat(\"Optimal Lambda:\", opt_lambda, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal Lambda: 0.01 \n```\n:::\n\n```{.r .cell-code}\nfinal_model <- glmnet(x, y, alpha = 1, lambda = opt_lambda)\n\n\nx_test <- as.matrix(test[, -1])  \ny_test_pred <- predict(final_model, newx = x_test)\n\n\n\nrmse <- sqrt(mean((y_test_pred - test$WS)^2))\ncat(\"RMSE:\", rmse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 1.184433 \n```\n:::\n\n```{.r .cell-code}\ncoef <- coef(final_model)\nprint(coef)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n26 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept) -1.357012e+01\nAge          1.287144e-02\nG            2.939427e-02\nMP           8.805988e-04\nFG           .           \nFGA         -1.688196e-03\nX2P          .           \nX2PA         .           \nX3P          1.048170e-03\nX3PA         .           \nFT           .           \nFTA          3.369140e-03\nORB          .           \nDRB          1.358432e-03\nTRB          2.883530e-03\nAST          6.304786e-03\nSTL          1.302799e-02\nBLK          8.091929e-03\nTOV         -2.051693e-02\nPF          -4.534926e-03\nPTS          3.951640e-03\nFG.          .           \nX2P.        -5.387480e+00\nFT.          .           \nTS.          2.349976e+01\neFG.         2.553410e+00\n```\n:::\n\n```{.r .cell-code}\nsummary(final_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Length Class     Mode   \na0         1     -none-    numeric\nbeta      25     dgCMatrix S4     \ndf         1     -none-    numeric\ndim        2     -none-    numeric\nlambda     1     -none-    numeric\ndev.ratio  1     -none-    numeric\nnulldev    1     -none-    numeric\nnpasses    1     -none-    numeric\njerr       1     -none-    numeric\noffset     1     -none-    logical\ncall       5     -none-    call   \nnobs       1     -none-    numeric\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}